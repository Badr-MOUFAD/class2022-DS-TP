{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d53d4fe3",
   "metadata": {},
   "source": [
    "# SUBJECT : ABOUT HUBER LOSS\n",
    "## Optimization with huber loss \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf4634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from scipy import linalg\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4748bebf",
   "metadata": {},
   "source": [
    "## Part 0: Why a robust regression model\n",
    "\n",
    "\n",
    "Let us consider the problem of regression from $n$ observations\n",
    "$x_i \\in \\mathbb{R}^{p}$,\n",
    "$1 \\leq i \\leq n$. We aim to learn a function:\n",
    "$$f: x \\in \\mathbb{R}^{p}\\mapsto y\\in\\mathbb{R}$$\n",
    "from the $n$ annotated training samples $(x_{i},y_{i})$ supposed i.i.d. from an unknown probability distribution on $\\mathbb{R}^p \\times \\mathbb{R}$. Once this function is learnt, it will be possible to use it to predict the label $y$ associated to a new sample $x$.\n",
    "\n",
    "The types of model we consider in this project are so-called *robust models* that can deal with samples corrupted by strong artifacts.\n",
    "\n",
    "Let's generate such a dataset in 1D to illustrate the problem when using the squared loss ($\\|\\cdot\\|^2$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d6b652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Generate toy data\n",
    "X, y = make_regression(n_samples=20, n_features=1, random_state=0,\n",
    "                       noise=4.0, bias=10.0)\n",
    "\n",
    "# Add an outlier\n",
    "X[0, 0] = 2.\n",
    "y[0] = 350\n",
    "\n",
    "# Fit the model\n",
    "reg = LinearRegression()\n",
    "reg.fit(X, y)\n",
    "\n",
    "# Visualize the model\n",
    "x = X[:, 0]\n",
    "y_pred = reg.coef_ * x + reg.intercept_\n",
    "\n",
    "plt.plot(x, y, 'b.')\n",
    "plt.plot(x, y_pred, 'g-', label=\"linear regression\")\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54728b43",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 0:</b>\n",
    "     <ul>\n",
    "       <li>Describe the issue you observe and suggest an explanation and a possible solution.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b33f739",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0e32b",
   "metadata": {},
   "source": [
    "## Huber Loss\n",
    "\n",
    "One version of the Huber function ($H_\\epsilon : \\mathbb{R} \\rightarrow \\mathbb{R}$) reads:\n",
    "\n",
    "$$\n",
    "    H_\\epsilon (x) = \\left\\{\n",
    "\t\\begin{aligned}\n",
    "\tx^2 & \\quad \\mathrm{ if } \\quad |x| < \\epsilon \\\\\n",
    "    2 \\epsilon |x| - \\epsilon^2 & \\quad \\mathrm{ otherwise }\n",
    "\t\\end{aligned}\n",
    "    \\right.\n",
    "$$\n",
    "\n",
    "Working in a regression setting, the Huber loss between 2 targets $y$ and $y'$ reads:\n",
    "\n",
    "$$\n",
    "    \\mathcal{L}(y, y') = H_\\epsilon (y - y')\n",
    "$$\n",
    "\n",
    "Here is an implemention of the Huber function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4812a01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "epsilon = 1.\n",
    "\n",
    "## fill in \n",
    "def huber(x, epsilon=epsilon):\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e87ac4b5",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 1:</b>\n",
    "     <ul>\n",
    "       <li>Plot the Huber function vs. the squared function ($x \\rightarrow x^2$) vs. the absolute value function ($x \\rightarrow |x|$) between -3 and 3 using $\\epsilon = 1$</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24001ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe070c8",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 2:</b>\n",
    "     <ul>\n",
    "       <li>Justify the convexity of the Huber function as defined above.</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9389a5f",
   "metadata": {},
   "source": [
    "INSERT YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53540719",
   "metadata": {},
   "source": [
    "we have: $H_\\varepsilon(X) =\\begin{cases}x^2 & \\mid x\\mid   <  \\varepsilon\\\\2\\varepsilon\\mid x\\mid - \\varepsilon^2 & otherwise\\end{cases} $\n",
    "\n",
    "so: $H'_\\varepsilon(X) =\\begin{cases}2x & \\mid x\\mid   <  \\varepsilon\\\\2\\varepsilon sign(x)  &  otherwise\\end{cases}$\n",
    "\n",
    "$\\Rightarrow H''_\\varepsilon(X) = \\begin{cases}2 & \\mid x\\mid   <  \\varepsilon\\\\0  &  otherwise\\end{cases}$\n",
    "\n",
    "\n",
    "This means that $ 0\\leq H''_\\varepsilon(X) \\leq 2$. So the Huber function is convex and 2-smooth. As a result, we propose the value 2 for the Lipschitz constant of the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5d7a525",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-success\">\n",
    "    <b>QUESTION 3:</b>\n",
    "     <ul>\n",
    "       <li>Write a function that computes the gradient of the Huber loss.</li>\n",
    "    </ul>\n",
    "</div>\n",
    "\n",
    "**Remark:** You will use the `scipy.optimize.check_grad` function to assess the validity of your result. You will need to test your gradient in both the linear and quadratic regions of the Huber function (not just in one location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f23727a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### FILL IN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de3536",
   "metadata": {},
   "source": [
    "Let us define the cost function associated to the empirical risk with some regularization function $\\mathcal{R}$:\n",
    "\n",
    "$$\n",
    "    (\\mathcal{P}_{f,\\mathcal{R}}):\n",
    "\t\\begin{aligned}\n",
    "\t\\min_{w \\in \\mathbb{R}^p, b \\in \\mathbb{R}} \\quad \\frac{1}{n}Â \\sum_{i=1}^n f(y_i - x_i^\\top w - b) + \\lambda \\mathcal{R}(w) \\enspace ,\n",
    "\t\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $f$ is a scalar function defining the loss (Huber, squared, absolute etc.). The variable $b$ is the bias or intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ffb2a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_l_bfgs_b\n",
    "\n",
    "lbda = 0.01\n",
    "\n",
    "def pobj_l2(params, X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    return np.mean(huber(y - np.dot(X, w) - b, epsilon=epsilon)) + lbda * np.sum(w ** 2)\n",
    "\n",
    "def grad_huber_l2(params,X=X, y=y, lbda=lbda ,epsilon=epsilon):\n",
    "    w = params[1:]\n",
    "    b = params[0]\n",
    "    grad_w = (-np.mean(X.T@grad_huber(y - np.dot(X, w) - b, epsilon=epsilon))+ 2*lbda*w)\n",
    "    grad_b = -np.mean(grad_huber(y - np.dot(X, w) - b, epsilon=epsilon))\n",
    "    return np.concatenate([grad_b, grad_w], axis=None)\n",
    "\n",
    "def huber_lbfgs_l2(X=X, y=y, lbda=lbda, epsilon=epsilon):\n",
    "    w0 = np.zeros(X.shape[1]+1)\n",
    "    params,_,_  = fmin_l_bfgs_b(pobj_l2, w0, grad_huber_l2)\n",
    "    return params\n",
    "\n",
    "\n",
    "# FILL IN  (for visualization)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fdf761",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ac9dfe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06687b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e49d961",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4f5014",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2010b40b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a0dde2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8f576",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
